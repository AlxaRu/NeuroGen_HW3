{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMaRpeoelKkzNABjOmytnTH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlxaRu/NeuroGen_HW3/blob/main/NeuroGen_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5QHkz7kcOpX",
        "outputId": "5bf1f9c2-dc97-4003-a94d-6e01560c20a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysam in /usr/local/lib/python3.11/dist-packages (0.22.1)\n"
          ]
        }
      ],
      "source": [
        "# Dan Udi Shefler - 324959147\n",
        "# Alexandra Rukban - 203615752\n",
        "!pip install pysam\n",
        "\n",
        "### import libraries, input parameters, initilize dictionaries and arrays\n",
        "import pysam\n",
        "from  scipy.stats import binom\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "### start of input\n",
        "# quality score 30 corresponds to 1/1000 error rate, or 1e-3 sequencing error\n",
        "minimum_qaulity_score = 30\n",
        "P = 1e-3 #  propability of sequencing error given qaulity score of 30\n",
        "max_mapping_score = 42 # the maximal mapping quality means unique alignment, with bowtie2 this number is 42\n",
        "p_cutoff = 0.05 # the cutoff for significance, note that bonferroni correction is used below\n",
        "bam_file_to_read = \"aligned_RNAreads_sorted.bam\"\n",
        "output_file = \"editing_sites_spectrin.txt\"\n",
        "length_of_read = 100\n",
        "total_orf_size = 16607506\n",
        "### end of input\n",
        "\n",
        "# initilize dictionary for counts per letter\n",
        "counts_per_letter = {}\n",
        "\n",
        "# initilize dictionary for type of modification\n",
        "number_of_modifications = {}\n",
        "type_of_modifications = ['AC','AG','AT','CA','CG','CT','GA','GC','GT','TA','TC','TG']\n",
        "for x in type_of_modifications:\n",
        "    number_of_modifications[x] = 0\n",
        "\n",
        "# initilize numpy array for position of mismatch in the read\n",
        "position_of_mismatch_inside_the_read = np.zeros(length_of_read)\n",
        "\n",
        "# initilize numpy array for position of match in the read\n",
        "position_of_match_inside_the_read = np.zeros(length_of_read)\n",
        "\n",
        "# Create a dictionary to store Amino Acid changes\n",
        "AAChanges = {'Synonymous': 0}\n",
        "\n",
        "# Create a codon table dictionary to convert sequence to acid name\n",
        "codon_table = {\n",
        "    'UUU': 'F', 'UUC': 'F', 'UUA': 'L', 'UUG': 'L',\n",
        "    'UCU': 'S', 'UCC': 'S', 'UCA': 'S', 'UCG': 'S',\n",
        "    'UAU': 'Y', 'UAC': 'Y', 'UAA': '*', 'UAG': '*',\n",
        "    'UGU': 'C', 'UGC': 'C', 'UGA': '*', 'UGG': 'W',\n",
        "    'CUU': 'L', 'CUC': 'L', 'CUA': 'L', 'CUG': 'L',\n",
        "    'CCU': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n",
        "    'CAU': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n",
        "    'CGU': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n",
        "    'AUU': 'I', 'AUC': 'I', 'AUA': 'I', 'AUG': 'M',\n",
        "    'ACU': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n",
        "    'AAU': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n",
        "    'AGU': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n",
        "    'GUU': 'V', 'GUC': 'V', 'GUA': 'V', 'GUG': 'V',\n",
        "    'GCU': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n",
        "    'GAU': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n",
        "    'GGU': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function definitions\n",
        "def find_codon(sequence, changePos, originalSeq, change):\n",
        "    # Make sure that we're using RNA bases\n",
        "    if originalSeq == 'T':\n",
        "        originalSeq = 'U'\n",
        "    if change == 'T':\n",
        "        change = 'U'\n",
        "    # Find the start of the codon\n",
        "    codon_start = (changePos // 3) * 3\n",
        "    # Extract the original codon\n",
        "    original_codon = sequence[codon_start:codon_start + 3].replace('T', 'U')\n",
        "    codonLst = list(original_codon)\n",
        "    codonLst[changePos % 3] = originalSeq\n",
        "    original_codon = ''.join(codonLst)\n",
        "\n",
        "    if len(codonLst) != 3:\n",
        "        print(codonLst)\n",
        "    # Create a list from the original codon for mutable string\n",
        "    changed_codon_list = codonLst\n",
        "    # Update the specific base in the codon\n",
        "    changed_codon_list[(changePos % 3)] = change\n",
        "    # Convert list back to string\n",
        "    changed_codon = ''.join(changed_codon_list)\n",
        "\n",
        "    return original_codon, changed_codon"
      ],
      "metadata": {
        "id": "zsxXQRrtdma2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### use pysam to examine all the reads aligned for each position of each transcript and detect editing sites\n",
        "samfile = pysam.AlignmentFile(bam_file_to_read, \"rb\") # read the indexed and sorted bam file using pysam\n",
        "file2 = open(output_file, \"w\") # file in which the detected editing sited will be listed\n",
        "\n",
        "# Read the transcriptome and output relevant data\n",
        "pattern = re.compile(\n",
        "    r'>(comp[^\\s]+).*?OrfStart\\s+(\\d+)\\s+OrfEnd\\s+(\\d+).*?\\n([ATCG\\n]+)',\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "# Open the data file and read its content\n",
        "with open ('pealeii.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Find all matches in the data\n",
        "matches = pattern.findall(data)\n",
        "\n",
        "# Create a dictionary to store the name of the gene, their transcripts and orf ranges (start and end)\n",
        "orf_dictionary = {}\n",
        "for match in matches:\n",
        "    gene_name, orf_start, orf_end, transcipt = match\n",
        "    orf_start = int(orf_start)\n",
        "    orf_end = int(orf_end)\n",
        "\n",
        "    # Remove newlines from the transcripts\n",
        "    transcipt = transcipt.replace('\\n', '')\n",
        "\n",
        "    # Store the transcript and the range of the transcript in the dictionary\n",
        "    orf_dictionary[gene_name] = [transcipt, [orf_start, orf_end]]\n",
        "\n",
        "for transcript_name in orf_dictionary.keys():\n",
        "    # run over all transcripts\n",
        "    file2.write(\"\\nname of transcript = %s\\n\" % transcript_name) # list the current transcript\n",
        "    # run over all positions of the transcript and fetch the reads aligned to it\n",
        "    # only fetch reads that have the maximal mapping score (i.e. unique alignment)\n",
        "    for pileupcolumn in samfile.pileup(transcript_name,min_mapping_quality = max_mapping_score):\n",
        "        position = pileupcolumn.pos\n",
        "        # Make sure the positions are in our reading frame according to pealeii.txt\n",
        "        if position >= orf_start and position <= orf_end:\n",
        "            # initilize the dictionary counts_per_letter\n",
        "            type_of_letters=['A','C','G','T']\n",
        "            for x in type_of_letters:\n",
        "                counts_per_letter[x] = 0\n",
        "            # run over all reads that align against the transcript in the given position\n",
        "            for pileupread in pileupcolumn.pileups:\n",
        "                # query position is None if is_del or is_refskip is set\n",
        "                if not pileupread.is_del and not pileupread.is_refskip:\n",
        "                    # demand minimum quality score for the aligned base\n",
        "                    if pileupread.alignment.query_qualities[pileupread.query_position] >= minimum_qaulity_score:\n",
        "                        # detect the base\n",
        "                        letter = pileupread.alignment.query_sequence[pileupread.query_position]\n",
        "                        # count how many times each base was detected\n",
        "                        counts_per_letter[letter] = counts_per_letter[letter] + 1\n",
        "            # sort the counts per base to detect the base most frequent and the second base i.e. the modification\n",
        "            sorted_dict = dict(sorted(counts_per_letter.items(),\n",
        "                                    key=lambda item: item[1],\n",
        "                                    reverse=True)) # sort descending\n",
        "            i = 0 # a running number to detect the first and second most frequent bases\n",
        "            N = 0 # total number of reads aligned to the first and second most frequent bases\n",
        "            for key, value in sorted_dict.items():\n",
        "                i = i + 1\n",
        "                if i == 1:\n",
        "                    N = N + value\n",
        "                    first_letter = key # the most frequent base\n",
        "                elif i == 2:\n",
        "                    X = value # the number of times the modified base appeared\n",
        "                    N = N + value\n",
        "                    second_letter = key # the second most frequent base\n",
        "                    modification = first_letter + second_letter # the modification, for example AG (i.e A changed to G)\n",
        "                else:\n",
        "                    break\n",
        "            # given a rate of sequencing error P, total number of counts N, and modified counts X \\\n",
        "            # the probability of event being a sequencing error is 1 - binomial_CDF(X - 1, N , P) \\\n",
        "            # this is the same as binomial_CDF(N - X , N, 1 - P) (but the latter is more accurate due to rounding errors)\n",
        "            prob = binom.cdf(N - X, N, 1 - P)\n",
        "            # bonferroni correction p_cutoff divided by the overall number of tests\n",
        "            if prob < (p_cutoff / total_orf_size):\n",
        "                # print the total number of reads aligned for the specfic location inside the transcript\n",
        "                file2.write(\"\\ncoverage at base %s = %s\" % (pileupcolumn.pos, pileupcolumn.n))\n",
        "                # sort the number of bases and print them to file\n",
        "                for key, value in sorted_dict.items():\n",
        "                    file2.write('\\ncounts of base %s = %s' % (key,str(value)))\n",
        "                file2.write('\\nX is %s and N is %s' % (str(X),str(N))) # print X and N, as defined above\n",
        "                part1='\\nP-value for sequencing error is\\t'\n",
        "                part2=\"{:.2e}\".format(prob)\n",
        "                part3='\\n'\n",
        "                file2.write(part1 + part2 + part3) # print the P-value\n",
        "                startIdx = orf_dictionary[transcript_name][1][0] - 1\n",
        "                endIdx = orf_dictionary[transcript_name][1][1]\n",
        "                if (position - 1) in range(startIdx, endIdx):\n",
        "                    original, new = find_codon(orf_dictionary[transcript_name][0][startIdx:endIdx],\n",
        "                                               position - startIdx - 1,\n",
        "                                               list(sorted_dict.keys())[0], list(sorted_dict.keys())[1])\n",
        "                    original, new = codon_table[original], codon_table[new]\n",
        "                    if (original != new):\n",
        "                        file2.write('Amino Acid change: %s to %s\\n' % (original, new))\n",
        "                        if original + new in AAChanges.keys():\n",
        "                            AAChanges[original + new] += 1\n",
        "                        else:\n",
        "                            AAChanges[original + new] = 1\n",
        "                    else:\n",
        "                        file2.write('Amino Acid unchanged: %s\\n' % original)\n",
        "                        AAChanges['Synonymous'] += 1\n",
        "                else:\n",
        "                    file2.write('Detected change not in ORF\\n')\n",
        "\n",
        "                # update the number of modifications\n",
        "                number_of_modifications[modification]=number_of_modifications[modification] + 1\n",
        "\n",
        "                # check the location of the match and mismatch in the read\n",
        "                #  to estimate if the modifications came from the reads ends\n",
        "                # run over all reads that align against the transcript in the given position\n",
        "                for pileupread in pileupcolumn.pileups:\n",
        "                    # query position is None if is_del or is_refskip is set\n",
        "                    if not pileupread.is_del and not pileupread.is_refskip:\n",
        "                        # demand minimum quality score for the aligned base\n",
        "                        if pileupread.alignment.query_qualities[pileupread.query_position] >= minimum_qaulity_score:\n",
        "                            # detect the base\n",
        "                            letter = pileupread.alignment.query_sequence[pileupread.query_position]\n",
        "                            if (letter == first_letter): # these are reads that show the match\n",
        "                                position_of_match_inside_the_read[pileupread.query_position] = \\\n",
        "                                position_of_match_inside_the_read[pileupread.query_position] + 1\n",
        "                            elif (letter == second_letter): # these are reads that show the mismatch\n",
        "                                position_of_mismatch_inside_the_read[pileupread.query_position] = \\\n",
        "                                position_of_mismatch_inside_the_read[pileupread.query_position] + 1\n",
        "                break\n",
        "        else:\n",
        "            print(\"Position \", position, \" in gene \", transcript_name, \" is out of range. \")\n",
        "samfile.close()\n",
        "file2.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Ejio7o1ndn32",
        "outputId": "43d7bfc7-8df4-4bdb-955c-33813029dfea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] could not open alignment file `aligned_RNAreads_sorted.bam`: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2a4e71aaf074>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### use pysam to examine all the reads aligned for each position of each transcript and detect editing sites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msamfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlignmentFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbam_file_to_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# read the indexed and sorted bam file using pysam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfile2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# file in which the detected editing sited will be listed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read the transcriptome and output relevant data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pysam/libcalignmentfile.pyx\u001b[0m in \u001b[0;36mpysam.libcalignmentfile.AlignmentFile.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pysam/libcalignmentfile.pyx\u001b[0m in \u001b[0;36mpysam.libcalignmentfile.AlignmentFile._open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] could not open alignment file `aligned_RNAreads_sorted.bam`: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "iFkHcHhjkckU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nv_QkNPEfnqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plot the results\n",
        "# plot the number of modifications per modification type sorted from highest to lowest\n",
        "sorted_dict = dict( sorted(number_of_modifications.items(),\n",
        "                        key = lambda item: item[1],\n",
        "                        reverse = True)) # sort descending\n",
        "plt.bar(range(len(sorted_dict)), list(sorted_dict.values()), align = 'center')\n",
        "plt.xticks(range(len(sorted_dict)), list(sorted_dict.keys()))\n",
        "plt.ylabel('Number of modifications')\n",
        "plt.xlabel('Modification type')\n",
        "plt.show()\n",
        "# plot the number of mismatches per position inside the read from base 1 to end\n",
        "plt.plot(list(range(0, length_of_read)),\\\n",
        "         position_of_mismatch_inside_the_read / sum(position_of_mismatch_inside_the_read),\\\n",
        "         label = \"mismatch\")\n",
        "# on the same figure, plot the number of matches per position inside the read from base 1 to end\n",
        "plt.plot(list(range(0, length_of_read)),\\\n",
        "         position_of_match_inside_the_read / sum(position_of_match_inside_the_read),\\\n",
        "         label = \"match\")\n",
        "plt.ylabel('Fraction')\n",
        "plt.xlabel('LocationInsideTheRead')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Sort amino changes dictionary by values and print\n",
        "AAChanges = dict(sorted(AAChanges.items(), key=lambda item: item[1], reverse=True))\n",
        "for key in AAChanges:\n",
        "    print(key, \": \", AAChanges[key])\n",
        "samfile.close()"
      ],
      "metadata": {
        "id": "vZsiL1NddrQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}